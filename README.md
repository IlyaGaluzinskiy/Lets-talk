## Let's talk - ASL (American Sign Language) recognition

### Let's talk was build in order to recogize and translate ASL gestures in real-time using computer vision (Project was build in two weeks)
- Let's talk currently able to recognize 39 gestures (26 letters, 10 words, 3 special signs)
- Let's talk was build using MediaPipe, OpenCV, Keras, Qt
- LSTM model for sign recognition was build using Keras and trained on dataset, that was collected during development (currently includes more than 5000 samples)
- user guide and list of available signs are located in the UI
- required libraries can be installed using requirements.txt file

Folder "for_model_training" includes scripts, that will allow you to create your own dataset.
![Ilya (2)](https://user-images.githubusercontent.com/74296883/138877590-9a5fcda6-3d18-4f26-a1e3-8514ba03b43e.gif)
