## Let's talk - ASL (American Sign Language) recognition

### Let's talk was build in order to recogize and translate ASL gestures in real-time using computer vision (Project was build in two weeks)
- Let's talk currently able to recognize 39 gestures (26 letters, 10 words, 3 special signs)
- Let's talk was build using MediaPipe, OpenCV, Keras, Qt
- LSTM model for sign recognition was build using Keras and trained on dataset, that was collected during development (currently includes more than 5000 samples)
- user guide and list of available signs are available in the UI
- required libraries can be install using requirements.txt file

Folder "for_model_training" includes scripts, that will allow you to create your own dataset.
